package linkchecker

import (
	"fmt"
	"io"
	"net/http"
	"net/url"
	"os"
	"strings"
	"sync"
	"time"

	"github.com/antchfx/htmlquery"
)

type Result struct {
	ResponseCode int
	Url          string
	ReferringURL string
	Error        error
}

type LinkChecker struct {
	HTTPClient *http.Client
	Wg         sync.WaitGroup
	Results    []Result
	output     io.Writer
	errorLog   io.Writer
	Scheme     string
	Domain     string
	RChannel   chan Result
}

type Option func(*LinkChecker) error

func WithOutput(output io.Writer) Option {
	return func(l *LinkChecker) error {
		l.output = output
		return nil
	}
}

func WithErrorLog(errorLog io.Writer) Option {
	return func(l *LinkChecker) error {
		l.errorLog = errorLog
		return nil
	}
}

func NewLinkChecker(opts ...Option) (*LinkChecker, error) {

	linkchecker := &LinkChecker{
		HTTPClient: &http.Client{Timeout: 10 * time.Second},
		output:     os.Stdout,
		errorLog:   os.Stderr,
	}

	for _, o := range opts {
		o(linkchecker)
	}

	return linkchecker, nil
}

func (l *LinkChecker) Check(site string) ([]Result, error) {

	results := make(chan Result)
	go l.ReceiveResultChannel(results)

	// get details from url
	url, err := url.Parse(site)
	if err != nil {
		return nil, err
	}
	if url.Scheme == "" || url.Host == "" {
		return nil, fmt.Errorf("invalid URL %q", url)
	}

	// needed for href w/o https://... used in ParseBody
	l.Scheme, l.Domain = url.Scheme, url.Host

	//l.Crawl(site, results)
	l.Crawl(site, results)

	// block here until all wait groups handled
	l.Wg.Wait()
	return l.Results, nil
}

func (l LinkChecker) Crawl(url string, results chan<- Result) error {

	fmt.Println(url)
	resp, err := l.HTTPClient.Get(url)
	if err != nil {
		return fmt.Errorf("unable to perform get on %s,%s", url, err)
	}

	defer resp.Body.Close()

	result := Result{
		Url:          url,
		ResponseCode: resp.StatusCode,
	}

	subsites, err := l.ParseBody(resp.Body)
	if err != nil {
		return err
	}

	for _, subsite := range subsites {

		// add to wait group
		l.Wg.Add(1)
		//go l.Get(url, subsite, results)
		go l.Crawl(subsite, results)

	}

	results <- result

	return nil
}

func (l LinkChecker) Get(referringUrl string, url string, results chan<- Result) {

	_, err := l.IsHeaderAvailable(url)
	if err != nil {
		result := Result{
			Url:          url,
			Error:        err,
			ReferringURL: referringUrl,
		}
		results <- result
		return

	}

	resp, err := l.HTTPClient.Get(url)
	if err != nil {
		fmt.Fprintln(l.output, err)
	}
	defer resp.Body.Close()

	result := Result{
		Url:          url,
		ResponseCode: resp.StatusCode,
	}
	results <- result

}

func (l *LinkChecker) ReceiveResultChannel(results <-chan Result) {

	for result := range results {
		l.Results = append(l.Results, result)
		l.Wg.Done()

	}

}

type Site struct {
	URL string
}

func (l LinkChecker) ParseBody(body io.Reader) ([]string, error) {

	doc, err := htmlquery.Parse(body)
	if err != nil {
		return nil, fmt.Errorf("unable to parse body, check if a valid io.Reader is being sent, %s", err)
	}

	list := htmlquery.Find(doc, "//a/@href")

	var sites []string
	var site string

	for _, n := range list {
		href := htmlquery.InnerText(n)

		switch {

		case strings.HasPrefix(href, "/"):
			url := fmt.Sprintf("%s://%s%s", l.Scheme, l.Domain, href)
			site = url

		case strings.HasPrefix(href, "https://"):
			site = href

		case strings.HasPrefix(href, "http://"):
			site = href

		}
		sites = append(sites, site)

	}

	return sites, nil
}

func (l LinkChecker) IsHeaderAvailable(url string) (bool, error) {

	resp, err := l.HTTPClient.Head(url)
	if err != nil {
		return false, err
	}

	defer resp.Body.Close()

	// req, err := http.NewRequest(http.MethodHead, url, nil)
	// if err != nil {
	// 	return false, err
	// }
	// _, err = l.HTTPClient.Do(req)

	// if err != nil {
	// 	return false, err
	// }

	return true, nil
}

// CLI

func RunCLI() {

	arg := os.Args[1:2]
	if arg[0] == "help" {
		help()
		os.Exit(0)
	}

	site := arg[0]

	l, err := NewLinkChecker()
	if err != nil {
		fmt.Fprintln(l.errorLog, err)
	}

	results, err := l.Check(site)
	if err != nil {
		fmt.Fprintln(l.errorLog, err)
	}

	for _, result := range results {
		fmt.Fprintln(l.output, result)
	}

}

func help() {
	fmt.Fprintln(os.Stderr, `
	Description:
	  linkchecker will crawl a site and return the status of each link on the site
	
	Parameters:
	  None
	
	Usage:
	./linkchecker https://example.com
	`)
}
